{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOSJJB1jfAT/NMmGFENUqdW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JyotiChowrasia05/PySpark_Steephan/blob/main/PySpark_Steephan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRdCB1xTA4au"
      },
      "outputs": [],
      "source": [
        "#!pip install PySpark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, trim, lower, upper, lit, length, \\\n",
        "    to_timestamp, to_date, coalesce, regexp_replace, when, isnan, isnull\n",
        "#from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType, DateType, TimestampType"
      ],
      "metadata": {
        "id": "XVOXnqlfF1JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MeteoriteApp\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Session Created Successfully!\")"
      ],
      "metadata": {
        "id": "QNYnM9m7BqLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the CSV file\n",
        "csv_file_path = \"/content/meteorite_landings_raw.csv\"\n",
        "\n",
        "df = spark.read.csv(\n",
        "    csv_file_path,\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")\n",
        "\n",
        "print(f\"\\nDataFrame created from '{csv_file_path}'\")"
      ],
      "metadata": {
        "id": "xV6tQLoBCAbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df)"
      ],
      "metadata": {
        "id": "1mmwd7RNEmku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "WUDN5pRuEbrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().show()"
      ],
      "metadata": {
        "id": "c6jW0OiXDqqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "jm7PsiUVEYSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data cleaning\n",
        "\n",
        "# Handle Missing Values (Nulls)\n",
        "df = df.na.fill({\"name\": \"na\"})\n",
        "\n",
        "# Removing Duplicate Rows (Entire Row Duplicates)\n",
        "print(f\"\\nOriginal count: {df.count()}\")\n",
        "df = df.dropDuplicates()\n",
        "print(f\"Count after dropping full row duplicates: {df.count()}\")\n",
        "\n",
        "print(\"\\n--- Final Cleaned DataFrame ---\")\n",
        "df.show(truncate=False)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "q18ald9jxWcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_checked = df.withColumn(\n",
        "    \"has_lead_trail_ws\",\n",
        "    (length(col(\"name\")) != length(trim(col(\"name\"))))\n",
        "    .alias(\"has_lead_trail_ws\")\n",
        ")\n",
        "\n",
        "print(\"\\nDataFrame with a flag for leading/trailing whitespace:\")\n",
        "df_checked.show()"
      ],
      "metadata": {
        "id": "9XQRd42sH_9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Triming the whitr space from name column\n",
        "df = df.withColumn(\"name\", trim(col(\"name\")))\n",
        "print(\"\\nDataFrame with 'name' column trimmed:\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "tcuBA97_FPKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df['year'])"
      ],
      "metadata": {
        "id": "fXewSnfiKc2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert Year column into a date type\n",
        "date_formats = [\n",
        "    \"dd/MM/yyyy hh:mm:ss a\",\n",
        "    \"M/d/yyyy H:mm\"\n",
        "]\n",
        "\n",
        "parsed_date_exp = [to_date(col(\"year\"), fmt) for fmt in date_formats]\n",
        "\n",
        "# Use coalesce to pick the first non-null parsed date\n",
        "df_converted = df.withColumn(\n",
        "    \"converted_date\",\n",
        "    coalesce(*parsed_date_exp) # The '*' unpacks the list into individual arguments\n",
        ")\n",
        "\n",
        "print(\"\\nDataFrame with converted date column (handling multiple formats):\")\n",
        "df_converted.show(truncate=False)\n",
        "df_converted.printSchema()\n"
      ],
      "metadata": {
        "id": "Ds7WyxWfFZvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overwrite the geolocation column to null values where the data is given as (0.000000, 0.000000)\n",
        "zero_coords_string = \"(0.000000, 0.000000)\"\n",
        "\n",
        "df_geo = df.withColumn(\n",
        "    \"geolocation\",\n",
        "    when(\n",
        "        (col(\"geolocation\") == zero_coords_string),\n",
        "         lit(None)\n",
        "    ).otherwise(col(\"geolocation\"))\n",
        ")\n",
        "\n",
        "print(\"\\nDataFrame after nullifying specific geolocation strings:\")\n",
        "df_geo.show(truncate=False)\n",
        "df_geo.printSchema()"
      ],
      "metadata": {
        "id": "7K1yUyaJOiO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the distinct values in column 'fall'\n",
        "df_distinct_fall = df.select(\"fall\").distinct()\n",
        "df_distinct_fall.show()"
      ],
      "metadata": {
        "id": "Jw2fq5hKmxxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average mass of Meteors which were fell\n",
        "df_fell = df.filter(\n",
        "    (col(\"fall\") == \"Fell\") &\n",
        "    col(\"mass (g)\").isNotNull() # Exclude null mass values from average\n",
        ")\n",
        "\n",
        "print(\"\\n The records (where column 'fall' has value 'Fell' and column mass is not null):\")\n",
        "print('\\n Total records:', df_fell.count())\n",
        "df_fell.show()\n",
        "\n",
        "avg_mass = df_fell.agg(avg(\"mass (g)\").alias(\"average_mass_fell\"))\n",
        "\n",
        "print(\"\\nAverage mass where 'fall' is 'Fell':\")\n",
        "avg_mass.show()"
      ],
      "metadata": {
        "id": "w1pg5td4ju-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average mass of Meteors which were Found\n",
        "df_found = df.filter(\n",
        "    (col(\"fall\") == \"Found\") &\n",
        "    col(\"mass (g)\").isNotNull() # Exclude null mass values from average\n",
        ")\n",
        "\n",
        "print(\"\\n The records (where column 'fall' has value 'Found' and column mass is not null):\")\n",
        "print('\\n Total records:', df_found.count())\n",
        "df_found.show()\n",
        "\n",
        "avg_mass_found = df_found.agg(avg(\"mass (g)\").alias(\"average_mass_found\"))\n",
        "\n",
        "print(\"\\nAverage mass where 'fall' is 'Found':\")\n",
        "avg_mass_found.show()"
      ],
      "metadata": {
        "id": "_IvEdD_vjuAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# marking flag_mass as True when the meteor mass is greater than the average mass\n",
        "\n",
        "df_overall_avg_mass = df.filter(col(\"mass (g)\").isNotNull()).agg(avg(col(\"mass (g)\")).alias(\"overall_avg_mass\"))\n",
        "df_overall_avg_mass.show()\n",
        "\n",
        "# Extracting the single average value from the 'df_overall_avg_mass' DataFrame\n",
        "# Using .collect()[0][0] to get the scalar value\n",
        "overall_avg_mass_value = df_overall_avg_mass.collect()[0][\"overall_avg_mass\"]\n",
        "print(f\"\\nThe Overall Average Meteor Mass: {overall_avg_mass_value}\")\n",
        "\n",
        "df_with_overall_avg = df.withColumn(\n",
        "    \"overall_avg_mass\",\n",
        "    lit(overall_avg_mass_value) # lit() to add a constant value to all rows\n",
        ")\n",
        "print(\"\\nDataFrame with the overall average mass column\")\n",
        "df_with_overall_avg.show()\n",
        "\n",
        "df_result = df_with_overall_avg.withColumn(\n",
        "    \"flag_mass \",\n",
        "    when(\n",
        "        (col(\"mass (g)\").isNotNull()) & (col(\"overall_avg_mass\").isNotNull()) &\n",
        "        (col(\"mass (g)\") > col(\"overall_avg_mass\")),\n",
        "        lit(True)\n",
        "    ).otherwise(lit(False)) # Set to False if not greater, or if mass/overall_avg is null\n",
        ")\n",
        "\n",
        "print(\"\\nFinal DataFrame with 'is_mass_greater_than_overall_avg' column:\")\n",
        "df_result.show()\n",
        "df_result.printSchema()"
      ],
      "metadata": {
        "id": "jIYLNWfqlhCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the SparkSession\n",
        "# spark.stop()\n",
        "# print(\"\\nSparkSession stopped.\")"
      ],
      "metadata": {
        "id": "8rWW02PZE_cG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}